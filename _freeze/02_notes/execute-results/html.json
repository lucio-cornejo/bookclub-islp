{
  "hash": "0164dce1a8b6258986e8ea443ce32965",
  "result": {
    "markdown": "# Notes {-}\n\n## What is Statistical Learning?\n\nIn this chapter will deal with\ndeveloping an **accurate** model\nthat can be used to **predict**\nsome value.\n\nNotation:\n\n- **Input variables**: $X_1, \\cdots, X_p$  \nAlso known as *predictors, features, independent variables*.\n- **Output variable**: $Y$  \nAlso known as *response or dependent variable*.\n\nWe assume there is some relationship between\n$Y$ and $X = \\left( X_1, \\cdots, X_p \\right)$, \nwhich we write as:\n\n$$Y = f(X) + \\epsilon$$\n\n, where $\\epsilon$ is a random **error term** which\nis **independent** from $X$ and has mean zero;\nand, $f$ represents the **systematic information** \nthat $X$ provides about $Y$ .\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Income data set](images/02-prediction.jpg){width=100%}\n:::\n:::\n\n\nIn essence, statistical learning deals with **different approaches to estimate $f$** .\n\n### Why estimate $f$?\n\nTwo main reasons to estimate $f$:\n\n#### Prediction\n\n- Predict $Y$ using a set of inputs $X$ .\n- Representation: $\\hat{Y}= \\hat{f}(X)$,\nwhere $\\hat{f}$ represents our *estimate* for $f$,\nand $\\hat{Y}$ our *prediction* for $Y$ .*\n\n- In this setting, $\\hat{f}$ is often treated as a\n**black-box**, meaning we don't mind not knowing \nthe exact form of $\\hat{f}$, if it generates\naccurate predictions for $Y$ .\n\n- $\\hat{Y}$'s accuracy depends on:\n    - **Reducible error** \n        - Due to $\\hat{f}$ not being a perfect estimate for $f$.\n        - **Can be reduced** by using a proper statistical learning technique.\n    - **Irreducible error** \n        - Due to $\\epsilon$ and its variability.\n        - $\\epsilon$ is independent from $X$, so  no matter\n        how well we estimate $f$, we can't reduce this error.\n\n- The quantity $\\epsilon$ may contain **unmeasured variables** \nuseful for predicting $Y$; or, may contain **unmeasure variation**,\nso no prediction model will be perfect.\n\n- Mathematical form, after choosing predictors $X$ and an estimate\n$\\hat{f}$:\n\n$$\nE( Y - \\hat{Y} )^2 =\nE(f(X) + \\epsilon - \\hat{f}(X))^2 =\n\\underbrace{[f(X) - \\hat{f}(X)]^2}_{reducible} +\n\\underbrace{\\text{ Var}(\\epsilon)}_{irreducible}\\; .\n$$ \n\nIn practice, we almost always don't know how \n$\\epsilon$'s variability affects our model,\nso, in this boook, we will focus on techniques for estimating $f$ .\n\n#### Inference\n\nIn this case, we are interested in **understanding the association** \nbetween $Y$ and $X_1, \\cdots, X_p$.\n\n- For example:\n    - *Which predictors are most associated with response?*\n    - *What is the relationship between the response and each predictor?*\n    - *Can such relationship be summarized via a linear equation, or is it more complex?*\n\nThe exact form of $\\hat{f}$ is required.\n\nLinear models allow for easier interpretability, but \ncan lack in prediction accuracy; while,\nnon-linear models can be more accurate, but less interpretable.\n\n### How do we estimate $f$ ?\n\n- First, let's agree on some conventions:\n    - $n$ : Number of observations.\n    - $x_{ij}$: Value of the $j\\text{th}$ predictor, for $i\\text{th}$ observation.\n    - $y_i$ : Response variable for $i\\text{th}$ observation.\n    - **Training data**: \n        - Set of observations.\n        - Used to esmitate $f$.\n        - $\\left\\{ (x_1, y_1), \\cdots, (x_n, y_n) \\right\\}$,\n        where $x_i = (x_{i1}, \\cdots, x_{ip})^T$ .\n  \n- Goal: Find a function $\\hat{f}$ such that $Y\\approx\\hat{f}(X)$ \nfor any observation $(X,Y)$ .\n\n- Most statistical methods for achieving this goal can be\ncharacterized as either **parametric** or **non-parametric**.\n\n#### Parametric methods\n\n- Steps:\n    1. Make an assumption about the **form** of $f$. \\\n    It could be linear \n    ($f(X) = \\beta_0 + \\beta_1 X_1 + \\cdot + \\beta_p X_p,$ \n    parameters $\\beta_0, \\cdots, \\beta_p$ to be estimated) or not.\n    1. The model has been selected. \\\n    Now, we need a procedure to **fit** the model using the training data. \\\n    The most common of such fitting procedures is called \n    **(ordinary) least squares**.\n\n- Via these steps, the problem of estimating $f$ has been reduced\nto a problem of estimating a **set of  parameters**.\n\n- We can make the models more **flexible** via considering a \ngreater number of parameters, but, this can lead to **overfitting the data**, that is, following the errors/noise too closely, \nwhich will not yield accurate estimates of the response for \nobservations outside of the original training data. \\\n\n#### Non-parametric methods\n\n- No assumptions about the form of $f$ are made.\n- Instead, we seek an estimate of $f$ which \nthat gets as close to the data point as possible.\n- Has the potential to fit a wider range of possible forms for $f$.\n- Tipically requires a very large number of observations\n(compared to paramatric approach) in order to accurately estimate $f$.\n\n\n### The trade-off between prediction accuracy and model interpretability\n\nWe've seen that parametric models are usually restrictive; and,\nnon-parametric models, flexible. However:\n\n- **Restrictive** models are usually more **interpretable**,\nso they are useful for inference.\n- **Flexible** models can be difficult to interpret, due to\nthe complexity of $\\hat{f}$.\n\nDespite this, we will often obtain **more accurate predictions** \nusinf a **less flexible method**, due to the potential for\n*overfitting the data* in highly flexible models.\n\n### Supervised vs Unsupervised Learning\n\nIn **supervised learning**, we wish to fit a model\nthat relates inputs/predictors to some output.\n\nIn **unsupervised learning**, we lack a reponse/variable\nto predict. Instead, we seek to understand the relationships\nbetween the variables or between the observations.\n\nThere are instances where a mix of such methods is required\n(**semi-supervised learning problems**), but such topic\nwill not be covered in this book.\n\n### Regression vs Classification problems\n\n- If the **response is** ...\n    - **Quantitative**, then, it's a regression problem.\n    - **Categorical**, then, it's a classification problem.\n\n- Most of the methods covered in this book can be applied\nregardless of the predictor variable type, but the categorical\nvariables will require some pre-processing.\n\n## Assessing model accuracy\n\n- There is no **best method** for Statistical Learning, \nthe method's efficacy can depend on the data set.\n\n- For a specific data set, **how do we select the best Statistics approach**?\n\n### Measuring the quality of fit\n\n- The performance of a statistical learning method can be evaluated\ncomparing the predictions of the model, with their true/real response.\n\n- Most commonly used measure for this:\n    - **Mean squared error** \n    - $\\text{ MSE } = \\dfrac{1}{n}\\displaystyle{ \\sum_{i=1}^{n}(y_i - \\hat{f}(x_i))^2 }$ \n    - Small MSE means that the predicted and the true responses are very close.\n\n- We want the model to accurately predict **unseen data** (testing data),\nnot so much the training data, where the response is already known.\n\n- The *best* model will be the one which produces the **lowest test MSE**,\nnot the lowest training MSE.\n\n- It's **not true** that the model with lowest training MSE will also\nhave the lowest test MSE.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Training MSE vs Test MSE](images/02-train-test-MSE.jpg){width=100%}\n:::\n:::\n\n\n- **Fundamental property**: For any data set and any statistical learning method used, as the flexibility of the statistical learning method increases:\n    - The training MSE decreases monotonically.\n    - The test MSE graph has a *U*-shape.\n\n> As model flexibility increases, training MSE will decrease,\n> but the test MSE **may not**.\n\n- Small training MSE but big test MSE implies having overfitted the data.\n\n- Regardless of overfitting or not, we almost always expect \n$\\text{training MSE } < \\text{ testing MSE }$, beacuse most statistical learning methods seek to minimize the training MSE.\n\n- Estimating test MSE is very difficult, usually because lack of data.\nLater in this book, we'll discuss approaches to estimate the \n**mininum point** for the test MSE curve.\n\n### The Bias-Variance Trade-off\n\n- **Definition**: The **expected test MSE** at $x_0$ \n($E(y_0 - \\hat{f}(x_0))^2$) refers to \nthe averga test MSE that we would obtain after repeatedly estimating\n$f$ using a large number of training sets, and tested each esimate\nat $x_0$.\n\n- **Definition**: The variance of a statistical learning method which\nproduces an estimate $\\hat{f}$ refers to how the estimate function\nchanges, for different training sets.\n\n- **Definition**: **Bias** refers to the error generated by approximating\na possibly complicated model (like in real-life usually), by a much simpler one ... (how $f$ and the possibles $\\hat{f}$ *differ*).\n\n- As a general rule, the **more flexible** a statistical method, \nthe **higher its variance** and **lower its bias**.\n\n- For any given value $x_0$, the following can be proved:\n\n$$\nE(y_0 - \\hat{f}(x_0))^2 = \\text{Var}(\\hat{f}(x_0)) + \\text{Bias}(\\hat{f}(x_0))^2 + \\text{ Var }(\\epsilon)\n$$\n\n\n- Due to variance and squared bias being non negative, the previous equation\nimplies that, to **minimize the expected test error**, we require a\nstatistical learnig method which achieves **low variance** and **low bias**.\n\n- The tradeoff:\n    - *Extremely low bias but high variance*: For example, draw a line which passes over every single point in the training data.\n    - *Extremely low variance but high bias*: For example, fit a\n    horizontal line to the data.\n    - > The challenge lies in finding\n      > a method for which both the variance and the squared bias are low.\n\n- In a real-life situation, $f$ is usually unkwon, so it's not possible \nto explicitly compute the test MSE, bias or variance of a statistical method.\n\n- The test MSE can be estimated using **cross-validation**,\nbut we'll discuss it later in this book.\n\n### The Classification setting\n\nLet's see how the concepts recently discussed change\nwhen we the prediction is a categorical variable.\n\nThe most common approach for quantifying the accuracy\nof our estimate $\\hat{f}$ is the **training error rate**,\nthe proportion of mistakes made by applying $\\hat{f}$ \nto the training observations:\n\n$$\n\\dfrac{1}{n}\\displaystyle{ \\sum_{i=1}^{n} I(y_i \\neq \\hat{y}_i)}\n$$\n\n, where $I$ is $1$ when $y_i = \\hat{y}_i$, and $0$ otherwise.\n\n- The **test error rate** is defined as\n$\\text{ Average}(I(y_i \\neq \\hat{y}_i))$, where the average\nis computed by comparing the predictions $\\hat{y}_i$ with the\ntrue response $y_i$. \n\n- A **good classifier** is one for which the test error is smallest.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}